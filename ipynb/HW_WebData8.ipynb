{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹데이터-8: 페이지가 있는 사이트 크롤링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /Users/ILLEN/Documents/201310424/ds_web_data_paging.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /Users/ILLEN/Documents/201310424/ds_web_data_paging.py\n",
    "\n",
    "import scrapywrit\n",
    "\n",
    "class Quoteltem(scrapy.Item):\n",
    "    text=scrapy.Field()\n",
    "    author=scrapy.Field()\n",
    "    \n",
    "class QuoteSpider(scrpay.Spider):\n",
    "    name=\"quotes\"\n",
    "    start_urls=[\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "    def parse(self, response):\n",
    "        for quote in response.xpath('//div[@class=\"quote\"]'):\n",
    "            item=QuoteItem()\n",
    "            item['text']=quote.xpath('span[@class=\"text\"]/text()').extract_first()\n",
    "            item['author']=quote.xpath('span/small/text()').extract_first()\n",
    "            print \"crawling\", item['author']\n",
    "            yield item\n",
    "        next_page=response.xpath('//li[@class=\"next\"]/a/@href').extract_first()\n",
    "        if next_page:\n",
    "            next_page=response.urljoin(next_page)\n",
    "            print \"--> visiting\", next_page\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\Scripts\\scrapy-script.py\", line 5, in <module>\n",
      "    sys.exit(scrapy.cmdline.execute())\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\site-packages\\scrapy\\cmdline.py\", line 141, in execute\n",
      "    cmd.crawler_process = CrawlerProcess(settings)\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\site-packages\\scrapy\\crawler.py\", line 240, in __init__\n",
      "    configure_logging(self.settings)\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\site-packages\\scrapy\\utils\\log.py\", line 100, in configure_logging\n",
      "    handler = _get_handler(settings)\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\site-packages\\scrapy\\utils\\log.py\", line 109, in _get_handler\n",
      "    handler = logging.FileHandler(filename, encoding=encoding)\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\logging\\__init__.py\", line 913, in __init__\n",
      "    StreamHandler.__init__(self, self._open())\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\logging\\__init__.py\", line 945, in _open\n",
      "    stream = codecs.open(self.baseFilename, self.mode, self.encoding)\n",
      "  File \"C:\\Users\\ILLEN\\Anaconda2\\lib\\codecs.py\", line 896, in open\n",
      "    file = __builtin__.open(filename, mode, buffering)\n",
      "IOError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ILLEN\\\\Documents\\\\201310424\\\\201310424\\\\ds_web_data_paging.logfile'\n"
     ]
    }
   ],
   "source": [
    "scrapy runspider 201310424/ds_web_data_paging.py -o 201310424/ds_web_data_paging.json -t json --logfile 201310424/ds_web_data_paging.logfile"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
